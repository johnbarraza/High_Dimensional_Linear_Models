{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Overfitting en Julia\n",
    "Este notebook implementa el cálculo de R², R² ajustado y R² fuera de muestra usando modelos polinomiales en Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, LinearAlgebra, Statistics\n",
    "using Plots\n",
    "using StatsBase\n",
    "\n",
    "Random.seed!(123)\n",
    "n = 1000\n",
    "X = rand(n)\n",
    "X = sort(X)\n",
    "ϵ = randn(n)\n",
    "y = exp.(4 .* X) .+ ϵ\n",
    "\n",
    "function r2(y, ŷ)\n",
    "    ss_res = sum((y .- ŷ).^2)\n",
    "    ss_tot = sum((y .- mean(y)).^2)\n",
    "    return 1 - ss_res/ss_tot\n",
    "end\n",
    "\n",
    "function r2_adjusted(r2::Float64, n::Int, k::Int)\n",
    "    if k >= n - 1\n",
    "        return NaN\n",
    "    else\n",
    "        return 1 - (1 - r2) * (n - 1) / (n - k - 1)\n",
    "    end\n",
    "end\n",
    "\n",
    "features_list = [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "r2_full = Float64[]\n",
    "r2_adj = Float64[]\n",
    "r2_out = Float64[]\n",
    "\n",
    "train_idx = sample(1:n, Int(round(0.75*n)), replace=false)\n",
    "test_idx = setdiff(1:n, train_idx)\n",
    "\n",
    "for p in features_list\n",
    "    X_poly = hcat([X.^i for i in 1:p]...)\n",
    "    β = X_poly \\ y\n",
    "    ŷ = X_poly * β\n",
    "    \n",
    "    r2_val = r2(y, ŷ)\n",
    "    push!(r2_full, r2_val)\n",
    "    push!(r2_adj, r2_adjusted(r2_val, n, p))\n",
    "\n",
    "    X_train, y_train = X_poly[train_idx, :], y[train_idx]\n",
    "    X_test, y_test = X_poly[test_idx, :], y[test_idx]\n",
    "\n",
    "    β_train = X_train \\ y_train\n",
    "    ŷ_test = X_test * β_train\n",
    "    push!(r2_out, r2(y_test, ŷ_test))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(features_list, r2_full, marker=:o, xscale=:log10,\n",
    "    xlabel=\"Número de features (escala log)\",\n",
    "    ylabel=\"R²\",\n",
    "    title=\"R² en toda la muestra\",\n",
    "    legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(features_list, r2_adj, marker=:o, xscale=:log10,\n",
    "    xlabel=\"Número de features (escala log)\",\n",
    "    ylabel=\"R² ajustado\",\n",
    "    title=\"R² ajustado\",\n",
    "    legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(features_list, r2_out, marker=:o, xscale=:log10,\n",
    "    xlabel=\"Número de features (escala log)\",\n",
    "    ylabel=\"R² fuera de muestra\",\n",
    "    title=\"R² en test\",\n",
    "    legend=false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "name": "julia",
   "version": "1.9.0",
   "mimetype": "application/julia",
   "file_extension": ".jl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "## Comentario de resultados\n",
  "\n",
  "En los tres gráficos se observa un patrón consistente en cuanto al incremento en el número de variables explicativas eleva rápidamente la capacidad explicativa del modelo hasta valores cercanos a uno, tanto en el R² de la muestra, como en el R² ajustado y el R² fuera de muestra.\n",
  "\n",
  "**R² en toda la muestra**:  \n",
  "A medida que se incorporan más regresores (X, X², …, Xᵖ), el modelo logra capturar de manera más precisa la relación no lineal entre X y Y. Este resultado es esperado porque las variables añadidas son relevantes, pues no se trata de ruido, sino de transformaciones con potencias directamente vinculadas con el proceso generador de datos.\n",
  "\n",
  "**R² ajustado**:  \n",
  "Normalmente, el R² ajustado penaliza la inclusión de regresores irrelevantes. Sin embargo, en este ejercicio, como todos los regresores contienen información relacionada con Y, la penalización no reduce significativamente el estadístico, que también se mantiene cercano a 1. Esto refleja que la ganancia de ajuste supera con claridad el costo de complejidad.\n",
  "\n",
  "**R² fuera de muestra**:  \n",
  "El desempeño en el conjunto de prueba confirma que no hay sobreajuste en sentido estricto. El modelo generaliza muy bien porque los regresores no son espurios; son potencias de X que reproducen la forma funcional subyacente de Y. En un escenario alternativo, donde hubiese regresores aleatorios o no correlacionados con la variable dependiente, el R² externo a la muestra caería cuando aumenta el número de variables, lo cual reflejaría el clásico problema de overfitting.\n"
 ]
}
