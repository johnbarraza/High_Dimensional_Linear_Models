{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eed5220",
   "metadata": {},
   "source": [
    "# 2. Overfitting (8 points)\n",
    "\n",
    "En esta sección, vamos a simular un proceso de generación de datos (DGP) para ilustrar el concepto de **overfitting** (sobreajuste). El objetivo es demostrar cómo las métricas de rendimiento de un modelo cambian a medida que añadimos más y más variables predictoras, incluso cuando estas variables adicionales son puro ruido y no tienen una relación real con la variable de respuesta.\n",
    "\n",
    "El DGP creará una variable de respuesta `y` que depende **únicamente** de una variable `x1`. Luego, entrenaremos modelos de regresión lineal con un número creciente de variables (`p`), donde todas las variables adicionales (`x2`, `x3`, ..., `xp`) son irrelevantes.\n",
    "\n",
    "Analizaremos tres métricas clave:\n",
    "1.  **R-cuadrado ($R^2$):** Mide la proporción de la varianza en `y` que es predecible a partir de las variables independientes *en la muestra de entrenamiento*.\n",
    "2.  **R-cuadrado ajustado ($R^2_{adj}$):** Similar al $R^2$, pero penaliza la adición de predictores que no mejoran significativamente el modelo.\n",
    "3.  **R-cuadrado fuera de muestra (Out-of-Sample $R^2$):** Mide el rendimiento del modelo en datos que no ha visto durante el entrenamiento (muestra de prueba). Esta es la prueba más fiable de la capacidad de generalización de un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcecddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy scikit-learn seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b30f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6990b",
   "metadata": {},
   "source": [
    "# 2. Definimos los parámetros de la simulación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c54356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametros\n",
    "n = 1000\n",
    "features = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "#generacion de datos, PGD\n",
    "np.random.seed(0)\n",
    "x = np.random.uniform(0, 1, n)\n",
    "epsilon_y = np.random.normal(0, 1, n)\n",
    "x.sort()\n",
    "x = x.reshape(-1, 1)\n",
    "# print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e97470",
   "metadata": {},
   "source": [
    "Ahora creamos y sin intercepto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486152b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
