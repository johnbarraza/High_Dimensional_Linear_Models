{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eed5220",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Overfitting en Python\n",
    "Este notebook implementa el cálculo de R², R² ajustado y R² fuera de muestra usando modelos polinomiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = 1000\n",
    "X = np.random.uniform(0, 1, n)\n",
    "X.sort()\n",
    "X = X.reshape(-1, 1)\n",
    "e = np.random.normal(0, 1, n).reshape(-1, 1)\n",
    "y = np.exp(4 * X) + e\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "def r2_manual(y, y_hat):\n",
    "    mse_val = mse(y, y_hat)\n",
    "    return 1 - mse_val / np.mean(y ** 2)\n",
    "\n",
    "def r2_adjusted(r2, n, k):\n",
    "    if k >= n-1:\n",
    "        return np.nan  # evitar divisiones extrañas\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - k - 1)\n",
    "\n",
    "features_list = [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "r2_full, r2_adj, r2_out, mse_full = [], [], [], []\n",
    "\n",
    "for p in features_list:\n",
    "    X_p = np.hstack([X ** i for i in range(1, p + 1)])\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_p, y)\n",
    "    y_hat = model.predict(X_p)\n",
    "\n",
    "    mse_full.append(mse(y, y_hat))\n",
    "    r2 = r2_manual(y, y_hat)\n",
    "    r2_full.append(r2)\n",
    "    r2_adj.append(r2_adjusted(r2, n, p))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_p, y, test_size=0.25, random_state=123)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    r2_out.append(r2_manual(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.plot(features_list, r2_full, marker=\"o\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Número de features (escala log)\")\n",
    "ax.set_ylabel(\"R²\")\n",
    "ax.set_title(\"R² en toda la muestra (manual)\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.plot(features_list, r2_adj, marker=\"o\", color=\"orange\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Número de features (escala log)\")\n",
    "ax.set_ylabel(\"R² ajustado\")\n",
    "ax.set_title(\"R² ajustado (manual)\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.plot(features_list, r2_out, marker=\"o\", color=\"green\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Número de features (escala log)\")\n",
    "ax.set_ylabel(\"R² fuera de muestra\")\n",
    "ax.set_title(\"R² en test (manual)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5356911",
   "metadata": {},
   "source": [
    "## Evaluación de resultados obtenidos\n",
    "\n",
    "En los tres gráficos se observa un patrón consistente en cuanto al incremento en el número de variables explicativas eleva rápidamente la capacidad explicativa del modelo hasta valores cercanos a uno, tanto en el R² de la muestra, como en el R² ajustado y el R² fuera de muestra.\n",
    "\n",
    "**R² en toda la muestra**:  \n",
    "A medida que se incorporan más regresores (X, X², …, Xᵖ), el modelo logra capturar de manera más precisa la relación no lineal entre X y Y. Este resultado es esperado porque las variables añadidas son relevantes, pues no se trata de ruido, sino de transformaciones con potencias directamente vinculadas con el proceso generador de datos.\n",
    "\n",
    "**R² ajustado**:  \n",
    "Normalmente, el R² ajustado penaliza la inclusión de regresores irrelevantes. Sin embargo, en este ejercicio, como todos los regresores contienen información relacionada con Y, la penalización no reduce significativamente el estadístico, que también se mantiene cercano a 1. Esto refleja que la ganancia de ajuste supera con claridad el costo de complejidad.\n",
    "\n",
    "**R² fuera de muestra**:  \n",
    "El desempeño en el conjunto de prueba confirma que no hay sobreajuste en sentido estricto. El modelo generaliza muy bien porque los regresores no son espurios; son potencias de X que reproducen la forma funcional subyacente de Y. En un escenario alternativo, donde hubiese regresores aleatorios o no correlacionados con la variable dependiente, el R² externo a la muestra caería cuando aumenta el número de variables, lo cual reflejaría el clásico problema de overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
