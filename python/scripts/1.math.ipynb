{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f57f338",
   "metadata": {},
   "source": [
    "## 1. Math : Demostración del Teorema de Frisch-Waugh-Lovell (FWL)\n",
    "\n",
    "El objetivo es probar que el estimador de Mínimos Cuadrados Ordinarios (MCO) para $\\theta_1$ en el modelo de regresión lineal completo es idéntico al estimador obtenido a través del procedimiento de dos pasos descrito.\n",
    "\n",
    "### El Modelo de Regresión Completo\n",
    "\n",
    " Consideremos el modelo de regresión lineal:\n",
    "$$y = X_1\\theta_1 + X_2\\theta_2 + \\epsilon$$\n",
    "Donde:\n",
    "* $y$ es el vector de resultados.\n",
    "* $X_1$ es la matriz de regresores de interés.\n",
    "* $X_2$ es la matriz de variables de control.\n",
    "\n",
    "Utilizando la fórmula para una matriz inversa particionada, se puede demostrar que el estimador MCO para $\\theta_1$ es:\n",
    "$$\\hat{\\theta}_1 = (X_1'M_2X_1)^{-1}(X_1'M_2y)$$\n",
    "Donde $M_2 = I - X_2(X_2'X_2)^{-1}X_2'$ es la **matriz de proyección residual** (o **matriz aniquiladora**) para $X_2$. Esta matriz es **simétrica** ($M_2' = M_2$) e **idempotente** ($M_2M_2 = M_2$).\n",
    "\n",
    "### El Procedimiento por Pasos\n",
    "\n",
    "Ahora, aplicamos el procedimiento descrito.\n",
    "\n",
    "**1. Obtener los residuos $\\hat{y}$**\n",
    "\n",
    "Regresamos $y$ vs $X_2$ y obtenemos los residuos. El vector de coeficientes para esta regresión es $\\hat{\\beta}_y = (X_2'X_2)^{-1}X_2'y$. Los residuos, $\\hat{y}$, son:\n",
    "$$\\hat{y} = y - X_2\\hat{\\beta}_y = y - X_2(X_2'X_2)^{-1}X_2'y$$\n",
    "Factorizando $y$, obtenemos:\n",
    "$$\\hat{y} = (I - X_2(X_2'X_2)^{-1}X_2')y = M_2y$$\n",
    "\n",
    "**2. Obtener los residuos $\\hat{X}_1$**\n",
    "\n",
    "De forma análoga, regresamos cada columna de la matriz $X_1$ en $X_2$ y obtenemos la matriz de residuos. La matriz de residuos $\\hat{X}_1$ es:\n",
    "$$\\hat{X}_1 = X_1 - X_2(X_2'X_2)^{-1}X_2'X_1$$\n",
    "Factorizando $X_1$, obtenemos:\n",
    "$$\\hat{X}_1 = (I - X_2(X_2'X_2)^{-1}X_2')X_1 = M_2X_1$$\n",
    "\n",
    "### La Regresión Final y la Prueba Formal\n",
    "$$M_2y = M_2X_1\\theta_1 + M_2X_2\\theta_2 + M_2\\epsilon$$\n",
    "$$\\hat{y} = \\hat{X}_1\\theta_1 + \\hat{\\epsilon}$$\n",
    "\n",
    "Finalmente, regresamos los residuos $\\hat{y}$ en los residuos $\\hat{X}_1$. El estimador MCO para esta regresión es, por definición:\n",
    "$$\\hat{\\theta}_{1,FWL} = (\\hat{X}_1'\\hat{X}_1)^{-1}\\hat{X}_1'\\hat{y}$$\n",
    "Sustituimos las definiciones de $\\hat{X}_1$ y $\\hat{y}$:\n",
    "$$\\hat{\\theta}_{1,FWL} = ((M_2X_1)'(M_2X_1))^{-1}((M_2X_1)'(M_2y))$$\n",
    "Usando las propiedades de simetría e idempotencia de $M_2$:\n",
    "* $(\\hat{X}_1'\\hat{X}_1) = (M_2X_1)'(M_2X_1) = X_1'M_2'M_2X_1 = X_1'M_2X_1$\n",
    "* $(\\hat{X}_1'\\hat{y}) = (M_2X_1)'(M_2y) = X_1'M_2'M_2y = X_1'M_2y$\n",
    "\n",
    "Sustituyendo estos resultados simplificados de vuelta en la fórmula:\n",
    "$$\\hat{\\theta}_{1,FWL} = (X_1'M_2X_1)^{-1}(X_1'M_2y)$$\n",
    "Esta expresión es **exactamente la misma** que la fórmula para $\\hat{\\theta}_1$ de la regresión completa. Con esto, hemos demostrado formalmente lo que se pedía:\n",
    "$$\\hat{\\theta}_{1}=(\\hat{X}_{1}'\\hat{X}_{1})^{-1}\\hat{X}_{1}'\\hat{y}$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
